{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we'll implement simple RNNs and LSTMs, then explore how gradients flow through these different networks.\n",
        "\n",
        "This notebook does not require a Colab GPU. If it's enabled, you can turn it off through Runtime -> Change runtime type. (This will make it more likely for you to get Colab GPU access later in the REAL_RNN_LSTM.ipynb problem.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports\n",
        "\n",
        "Note: the ipympl installation will require you to restart the colab runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install ipympl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import copy\n",
        "from typing import Callable\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interactive, widgets, Layout\n",
        "\n",
        "try:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "    RUNNING_IN_COLAB = True\n",
        "except ImportError:\n",
        "    RUNNING_IN_COLAB = False\n",
        "\n",
        "# === Reproducibility ===\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib ipympl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.A: Implementing a RNN Layer\n",
        "\n",
        "Consider using PyTorch's [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear). You can implement this with either one Linear layer or two. If you use two, remember that you only need to include a bias term for one of the linear layers.\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "$$h_t = \\sigma(W^h h_{t-1} + W^x x_t + b)$$\n",
        "\n",
        "where:\n",
        "- $W^h$ is the hidden-to-hidden weight matrix (hidden_size × hidden_size)\n",
        "- $W^x$ is the input-to-hidden weight matrix (input_size × hidden_size)  \n",
        "- $b$ is the bias vector (hidden_size)\n",
        "- $\\sigma$ is the nonlinearity (e.g., tanh)\n",
        "\n",
        "![RNN Unrolling](img/1a.png)\n",
        "\n",
        "*Figure: RNN unrolling across timesteps. Source: [Colah's Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNNLayer(nn.Module):\n",
        "    \"\"\"Single-layer Elman RNN implementation.\n",
        "    \n",
        "    Implements the recurrence relation:\n",
        "        h_t = σ(W_hh @ h_{t-1} + W_ih @ x_t + b)\n",
        "    \n",
        "    Learning Objectives:\n",
        "        - Understand how hidden state carries temporal information\n",
        "        - See the recurrence relation implemented in code\n",
        "        - Observe gradient flow through time via retain_grad()\n",
        "    \n",
        "    Attributes:\n",
        "        input_size: Dimension of input features at each timestep.\n",
        "        hidden_size: Dimension of hidden state (also output dimension).\n",
        "        nonlinearity: Activation function applied after linear transformation.\n",
        "    \n",
        "    References:\n",
        "        - Elman, J. L. (1990). \"Finding structure in time\"\n",
        "        - PyTorch nn.RNN for production implementation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_size: int,\n",
        "        nonlinearity: Callable[[torch.Tensor], torch.Tensor] = torch.tanh\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize a single RNN layer.\n",
        "        \n",
        "        Args:\n",
        "            input_size: Data input feature dimension.\n",
        "            hidden_size: RNN hidden state size (also the output feature dimension).\n",
        "            nonlinearity: Nonlinearity applied to the RNN output. Default: tanh.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.nonlinearity = nonlinearity\n",
        "        \n",
        "        ##############################################################################\n",
        "        # TODO: Initialize any parameters your class needs.                          #\n",
        "        ##############################################################################\n",
        "        \n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"RNN forward pass implementing h_t = σ(W_hh @ h_{t-1} + W_ih @ x_t + b).\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, input_size).\n",
        "        \n",
        "        Returns:\n",
        "            all_h: All hidden states of shape (batch_size, seq_len, hidden_size).\n",
        "                   Contains [h_1, h_2, ..., h_T] for each sequence.\n",
        "            last_h: Final hidden state of shape (batch_size, hidden_size).\n",
        "                    This is h_T, useful for sequence classification.\n",
        "        \"\"\"\n",
        "        hidden_states = []  # Will store [h_1, h_2, ..., h_T]\n",
        "        \n",
        "        ##############################################################################\n",
        "        # TODO: Implement the RNN forward step                                       #\n",
        "        # 1. Initialize h_0 with zeros: shape (batch_size, hidden_size)              #\n",
        "        # 2. Loop over timesteps t = 0, 1, ..., seq_len-1:                           #\n",
        "        #    - Compute h_{t+1} = σ(W_hh @ h_t + W_ih @ x_t + b)                       #\n",
        "        #    - Append h_{t+1} to hidden_states list                                  #\n",
        "        # 3. Set last_h = h_T (the final hidden state)                               #\n",
        "        ##############################################################################\n",
        "        \n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "\n",
        "        # Store hidden states for gradient analysis (used in visualization)\n",
        "        # hidden_states should contain T tensors, each of shape (batch_size, hidden_size)\n",
        "        self.store_hidden_states_for_grad(hidden_states)\n",
        "        \n",
        "        # Stack list of tensors into single tensor\n",
        "        # Shape transformation: List[T × (B, H)] → (B, T, H)\n",
        "        all_h = torch.stack(hidden_states, dim=1)  # (batch_size, seq_len, hidden_size)\n",
        "        return all_h, last_h\n",
        "\n",
        "    def store_hidden_states_for_grad(self, hidden_states: list[torch.Tensor]) -> None:\n",
        "        \"\"\"Store hidden states and enable gradient computation for analysis.\n",
        "        \n",
        "        Args:\n",
        "            hidden_states: List of hidden state tensors [h_1, ..., h_T].\n",
        "        \"\"\"\n",
        "        for h in hidden_states:\n",
        "            h.retain_grad()\n",
        "        self.h_list = hidden_states\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Cases\n",
        "\n",
        "If your implementation is correct, you should expect to see errors of less than 1e-4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Test 1: Single input, single timestep ===\n",
        "rnn = RNNLayer(input_size=1, hidden_size=1)\n",
        "\n",
        "rnn.load_state_dict({k: v * 0 + 0.1 for k, v in rnn.state_dict().items()})\n",
        "\n",
        "data = torch.ones((1, 1, 1))\n",
        "expected_out = torch.FloatTensor([[[0.1973753273487091]]])\n",
        "\n",
        "all_h, last_h = rnn(data)\n",
        "\n",
        "assert all_h.shape == expected_out.shape, (\n",
        "    f\"Shape mismatch: expected {expected_out.shape}, got {all_h.shape}\"\n",
        ")\n",
        "assert torch.all(torch.isclose(all_h, last_h)), (\n",
        "    \"For seq_len=1, all_h and last_h should be identical\"\n",
        ")\n",
        "print(f\"Expected: {expected_out.item():.6f}\")\n",
        "print(f\"Got:      {last_h.item():.6f}\")\n",
        "print(f\"Max error: {torch.max(torch.abs(expected_out - last_h)).item():.2e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Test 2: Multiple inputs, multiple timesteps, linear activation ===\n",
        "rnn = RNNLayer(\n",
        "    input_size=2,\n",
        "    hidden_size=3,\n",
        "    nonlinearity=lambda x: x\n",
        ")\n",
        "\n",
        "# Verify parameter count\n",
        "\n",
        "num_params = sum(p.numel() for p in rnn.parameters())\n",
        "assert num_params == 18, (\n",
        "    f\"Expected 18 parameters: W_ih(2×3=6) + W_hh(3×3=9) + b(3) = 18, \"\n",
        "    f\"but found {num_params}\"\n",
        ")\n",
        "print(f\"Parameter count correct: {num_params}\")\n",
        "\n",
        "rnn.load_state_dict({k: v * 0 - 0.1 for k, v in rnn.state_dict().items()})\n",
        "\n",
        "data = torch.FloatTensor([\n",
        "    [[0.1, 0.15], [0.2, 0.25], [0.3, 0.35], [0.4, 0.45]],\n",
        "    [[-0.1, -1.5], [-0.2, -2.5], [-0.3, -3.5], [-0.4, -0.45]]\n",
        "])\n",
        "\n",
        "expected_all_h = torch.FloatTensor([\n",
        "    [[-0.1250, -0.1250, -0.1250],\n",
        "     [-0.1075, -0.1075, -0.1075],\n",
        "     [-0.1328, -0.1328, -0.1328],\n",
        "     [-0.1452, -0.1452, -0.1452]],\n",
        "    [[ 0.0600,  0.0600,  0.0600],\n",
        "     [ 0.1520,  0.1520,  0.1520],\n",
        "     [ 0.2344,  0.2344,  0.2344],\n",
        "     [-0.0853, -0.0853, -0.0853]]\n",
        "])\n",
        "\n",
        "expected_last_h = torch.FloatTensor([\n",
        "    [-0.1452, -0.1452, -0.1452],\n",
        "    [-0.0853, -0.0853, -0.0853]\n",
        "])\n",
        "\n",
        "all_h, last_h = rnn(data)\n",
        "\n",
        "assert all_h.shape == expected_all_h.shape, (\n",
        "    f\"all_h shape mismatch: expected {expected_all_h.shape}, got {all_h.shape}\"\n",
        ")\n",
        "assert last_h.shape == expected_last_h.shape, (\n",
        "    f\"last_h shape mismatch: expected {expected_last_h.shape}, got {last_h.shape}\"\n",
        ")\n",
        "\n",
        "print(f\"Max error all_h:  {torch.max(torch.abs(expected_all_h - all_h)).item():.2e}\")\n",
        "print(f\"Max error last_h: {torch.max(torch.abs(expected_last_h - last_h)).item():.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.B: Implementing a RNN Regression Model\n",
        "\n",
        "Now we'll use the RNN layer in a regression model by adding a final linear layer on top:\n",
        "\n",
        "$$\\hat{y}_t = W^f h_t + b^f$$\n",
        "\n",
        "This transforms the hidden state at each timestep into a prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RecurrentRegressionModel(nn.Module):\n",
        "    \"\"\"RNN-based regression model with linear output layer.\n",
        "    \n",
        "    Architecture: Input → RNN/LSTM → Linear → Output\n",
        "    Computes: h_t = RNN(x_t, h_{t-1}), then ŷ_t = W^f @ h_t + b^f\n",
        "    \n",
        "    Attributes:\n",
        "        recurrent_net: The underlying RNN or LSTM module.\n",
        "        output_dim: Dimension of the output predictions.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        recurrent_net: nn.Module,\n",
        "        output_dim: int = 1\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize a simple RNN regression model.\n",
        "        \n",
        "        Args:\n",
        "            recurrent_net: An RNN or LSTM module (single or multi-layer).\n",
        "            output_dim: Feature dimension of the output predictions.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.recurrent_net = recurrent_net\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        ##############################################################################\n",
        "        # TODO: Initialize any parameters you need                                   #\n",
        "        # HINT: Use recurrent_net.hidden_size to find the hidden state size          #\n",
        "        ##############################################################################\n",
        "        \n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass through RNN and output layer.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, input_size).\n",
        "        \n",
        "        Returns:\n",
        "            out: Predictions of shape (batch_size, seq_len, output_dim).\n",
        "            all_h: Hidden states of shape (batch_size, seq_len, hidden_size).\n",
        "        \"\"\"\n",
        "        ##############################################################################\n",
        "        # TODO: Implement the forward step.                                          #\n",
        "        ##############################################################################\n",
        "        \n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "        return out, all_h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rnn = RecurrentRegressionModel(RNNLayer(2, 3), 4)\n",
        "\n",
        "num_params = sum(p.numel() for p in rnn.parameters())\n",
        "assert num_params == 34, f'expected 34 parameters but found {num_params}'\n",
        "\n",
        "rnn.load_state_dict({k: v * 0 - 0.1 for k, v in rnn.state_dict().items()})\n",
        "data = torch.FloatTensor([\n",
        "    [[0.1, 0.15], [0.2, 0.25], [0.3, 0.35], [0.4, 0.45]],\n",
        "    [[-0.1, -1.5], [-0.2, -2.5], [-0.3, -3.5], [-0.4, -0.45]]\n",
        "])\n",
        "expected_preds = torch.FloatTensor([\n",
        "    [[-0.0627, -0.0627, -0.0627, -0.0627],\n",
        "     [-0.0678, -0.0678, -0.0678, -0.0678],\n",
        "     [-0.0604, -0.0604, -0.0604, -0.0604],\n",
        "     [-0.0567, -0.0567, -0.0567, -0.0567]],\n",
        "    [[-0.1180, -0.1180, -0.1180, -0.1180],\n",
        "     [-0.1453, -0.1453, -0.1453, -0.1453],\n",
        "     [-0.1692, -0.1692, -0.1692, -0.1692],\n",
        "     [-0.0748, -0.0748, -0.0748, -0.0748]]\n",
        "])\n",
        "expected_all_h = torch.FloatTensor([\n",
        "    [[-0.1244, -0.1244, -0.1244],\n",
        "     [-0.1073, -0.1073, -0.1073],\n",
        "     [-0.1320, -0.1320, -0.1320],\n",
        "     [-0.1444, -0.1444, -0.1444]],\n",
        "    [[ 0.0599,  0.0599,  0.0599],\n",
        "     [ 0.1509,  0.1509,  0.1509],\n",
        "     [ 0.2305,  0.2305,  0.2305],\n",
        "     [-0.0840, -0.0840, -0.0840]]\n",
        "])\n",
        "preds, all_h = rnn(data)\n",
        "assert all_h.shape == expected_all_h.shape\n",
        "assert preds.shape == expected_preds.shape\n",
        "print(f'Max error all_h: {torch.max(torch.abs(expected_all_h - all_h)).item()}')\n",
        "print(f'Max error preds: {torch.max(torch.abs(expected_preds - preds)).item()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem 1.C: Dataset and Loss Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.C.i: Understanding the Dataset (no implementation needed)\n",
        "\n",
        "Inspect the code and plots below to visualize the dataset.\n",
        "\n",
        "![RNN Prediction Types](img/1c.png)\n",
        "\n",
        "*Figure: Different RNN prediction patterns - we focus on many-to-many and many-to-one.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_batch(\n",
        "    seq_len: int = 10,\n",
        "    batch_size: int = 1\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Generate synthetic time series data for cumulative average prediction.\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Length of each sequence.\n",
        "        batch_size: Number of sequences per batch.\n",
        "    \n",
        "    Returns:\n",
        "        data: Random values of shape (batch_size, seq_len, 1).\n",
        "        target: Cumulative averages of shape (batch_size, seq_len, 1).\n",
        "    \"\"\"\n",
        "    data = torch.randn(size=(batch_size, seq_len, 1))\n",
        "    sums = torch.cumsum(data, dim=1)\n",
        "    div = (torch.arange(seq_len) + 1).unsqueeze(0).unsqueeze(2)\n",
        "    target = sums / div\n",
        "    return data, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x, y = generate_batch(seq_len=10, batch_size=4)\n",
        "\n",
        "for i in range(4):\n",
        "    fig, ax1 = plt.subplots(1)\n",
        "    ax1.plot(x[i, :, 0])\n",
        "    ax1.plot(y[i, :, 0])\n",
        "    ax1.legend(['x', 'y'])\n",
        "    plt.title('Targets at all timesteps')\n",
        "    plt.show()\n",
        "\n",
        "for i in range(4):\n",
        "    fig, ax1 = plt.subplots(1)\n",
        "    ax1.plot(x[i, :, 0])\n",
        "    ax1.plot(np.arange(10), [y[i, -1].item()] * 10)\n",
        "    ax1.legend(['x', 'y'])\n",
        "    plt.title('Predict only at the last timestep')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.C.ii: Implement the Loss Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(\n",
        "    pred: torch.Tensor,\n",
        "    y: torch.Tensor,\n",
        "    last_timestep_only: bool = False\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Compute MSE loss for sequence prediction.\n",
        "    \n",
        "    Args:\n",
        "        pred: Model predictions of size (batch, seq_len, 1).\n",
        "        y: Targets of size (batch, seq_len, 1).\n",
        "        last_timestep_only: If True, compute loss only at final timestep.\n",
        "    \n",
        "    Returns:\n",
        "        loss: Scalar MSE loss between pred and true labels.\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the loss (HINT: look for pytorch's MSELoss function)       #\n",
        "    ##############################################################################\n",
        "    \n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tests\n",
        "\n",
        "You should see errors < 1e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = torch.FloatTensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
        "y = torch.FloatTensor([[-1.1, -1.2, -1.3], [-1.4, -1.5, -1.6]])\n",
        "loss_all = loss_fn(pred, y, last_timestep_only=False)\n",
        "loss_last = loss_fn(pred, y, last_timestep_only=True)\n",
        "assert loss_all.shape == loss_last.shape == torch.Size([])\n",
        "print(f'Max error loss_all: {torch.abs(loss_all - torch.tensor(3.0067)).item()}')\n",
        "print(f'Max error loss_last: {torch.abs(loss_last - torch.tensor(3.7)).item()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.D: Analyzing RNN Gradients\n",
        "\n",
        "**Key Insight:** When backpropagating through many timesteps, gradients can vanish or explode.\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{\\partial \\mathcal{L}}{\\partial h_T} \\frac{\\partial h_T}{\\partial W} + \\frac{\\partial \\mathcal{L}}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial W} + \\ldots + \\frac{\\partial \\mathcal{L}}{\\partial h_1} \\frac{\\partial h_1}{\\partial W}$$\n",
        "\n",
        "The later terms in this sum often end up with either very small magnitude (vanishing) or very large magnitude (exploding).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You do not need to understand the details of the GradientVisualizer class in order to complete this problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def biggest_eig_magnitude(matrix: torch.Tensor) -> float:\n",
        "    \"\"\"Compute the magnitude of the largest eigenvalue.\n",
        "    \n",
        "    Important because: |λ_max| > 1 → explosion, |λ_max| < 1 → vanishing\n",
        "    \n",
        "    Args:\n",
        "        matrix: A square matrix (n × n).\n",
        "    \n",
        "    Returns:\n",
        "        The magnitude of the eigenvalue with largest absolute value.\n",
        "    \"\"\"\n",
        "    h, w = matrix.shape\n",
        "    assert h == w, f'Matrix has shape {matrix.shape}, but eigenvalues can only be computed for square matrices'\n",
        "    eigs = torch.linalg.eigvals(matrix)\n",
        "    eig_magnitude = eigs.abs()\n",
        "    eigs_sorted = sorted([i.item() for i in eig_magnitude], reverse=True)\n",
        "    first_eig_magnitude = eigs_sorted[0]\n",
        "    return first_eig_magnitude\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GradientVisualizer:\n",
        "    \"\"\"Interactive visualization for RNN gradient flow analysis.\n",
        "    \n",
        "    Creates an interactive plot showing:\n",
        "    1. Hidden state magnitudes at each timestep\n",
        "    2. Gradient magnitudes (∂L/∂h_t) flowing backward through time\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rnn: RNNLayer, last_timestep_only: bool) -> None:\n",
        "        \"\"\"Initialize the gradient visualizer.\n",
        "        \n",
        "        Args:\n",
        "            rnn: RNN module to visualize.\n",
        "            last_timestep_only: If True, compute loss only at final timestep.\n",
        "        \"\"\"\n",
        "        self.rnn = rnn\n",
        "        self.last_timestep_only = last_timestep_only\n",
        "        self.model = RecurrentRegressionModel(rnn)\n",
        "        self.original_weights = copy.deepcopy(rnn.state_dict())\n",
        "\n",
        "        # Generate a single batch to be used repeatedly\n",
        "        self.x, self.y = generate_batch(seq_len=10)\n",
        "        print(f'Data point: x={np.round(self.x[0, :, 0].detach().cpu().numpy(), 2)}, y={np.round(self.y.squeeze().detach().cpu().numpy(), 2)}')\n",
        "\n",
        "    def plot_visuals(self):\n",
        "        \"\"\"Generate plots which will be updated in realtime.\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "        ax1.set_title('RNN Outputs')\n",
        "        ax1.set_xlabel('Unroll Timestep')\n",
        "        ax1.set_ylabel('Hidden State Norm')\n",
        "        ax1.set_ylim(-1, 5)\n",
        "        plt_1 = ax1.plot(np.arange(1, 11), np.zeros(10) + 1)\n",
        "        plt_1 = plt_1[0]\n",
        "\n",
        "        ax2.set_title('Gradients')\n",
        "        ax2.set_xlabel('Unroll Timestep')\n",
        "        ax2.set_ylabel('RNN dLoss/d a_t Gradient Magitude')\n",
        "        ax2.set_ylim((10**-6, 1e5))\n",
        "        ax2.set_yscale('log')\n",
        "        ax2.set_xticks(np.arange(10), np.arange(10, 0, -1))\n",
        "        plt_2 = ax2.plot(np.arange(10), np.arange(10) + 1)\n",
        "        plt_2 = plt_2[0]\n",
        "        self.fig = fig\n",
        "        self.plots = [plt_1, plt_2]\n",
        "        return plt_1, plt_2, fig\n",
        "\n",
        "    def update_plots(self, weight_val: float = 0, bias_val: float = 0) -> None:\n",
        "        \"\"\"Update visualization with scaled weights.\"\"\"\n",
        "        # Scale the original RNN weights by a constant\n",
        "        w_dict = copy.deepcopy(self.original_weights)\n",
        "        ##############################################################################\n",
        "        # TODO: Scale all W matrixes by weight_val, and all bias matrices by bias_val#\n",
        "        # If you're using PyTorch nn.Linear layers, you don't need to modify the code#\n",
        "        # provided, but if you're using custom layers, modify this block.            #\n",
        "        ##############################################################################\n",
        "        \n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "        self.rnn.load_state_dict(w_dict)\n",
        "\n",
        "        # Don't compute for LSTMs, which don't have behavior dependent on a single eigenvalue\n",
        "        if isinstance(self.rnn, RNNLayer):\n",
        "            ##############################################################################\n",
        "            # TODO: Set W = the weight which most affects exploding/vanishing gradients  #\n",
        "            # Hint: Call module.weight or module.bias on the module you want to use      #\n",
        "            # If you used a single Linear layer, slice a square matrix from it.          #\n",
        "            ##############################################################################\n",
        "            \n",
        "            ##############################################################################\n",
        "            #                               END OF YOUR CODE                             #\n",
        "            ##############################################################################\n",
        "            biggest_eig = biggest_eig_magnitude(W)\n",
        "            print(f' Biggest eigenvalue magnitude: {biggest_eig:.3}')\n",
        "\n",
        "        # Run model\n",
        "        pred, h = self.model(self.x)\n",
        "        loss = loss_fn(pred, self.y, self.last_timestep_only)\n",
        "        n_steps = len(h[0])\n",
        "\n",
        "        plt_1, plt_2 = self.plots\n",
        "\n",
        "        # Plot the hidden state magnitude\n",
        "        max_h = torch.linalg.norm(h[0], dim=-1).detach().cpu().numpy()\n",
        "        print('Max H', ' '.join([f'{num:.3}' for num in max_h]))\n",
        "        plt_1.set_data(np.arange(1, n_steps + 1), np.array(max_h))\n",
        "        \n",
        "        # Compute the gradient for the loss wrt the stored hidden states\n",
        "        grads = [torch.linalg.norm(num).item() for num in torch.autograd.grad(loss, self.rnn.h_list)][::-1]\n",
        "        print('gradients d Loss/d h_t', ' '.join([f'{num:.3}' for num in grads]))\n",
        "        plt_2.set_data(np.arange(n_steps), np.array(grads) + 1e-6)\n",
        "        self.fig.canvas.draw_idle()\n",
        "\n",
        "    def create_visualization(self):\n",
        "        \"\"\"Create interactive widget with sliders.\"\"\"\n",
        "        self.plot_visuals()\n",
        "        ip = interactive(\n",
        "            self.update_plots,\n",
        "            weight_val=widgets.FloatSlider(value=0, min=-5, max=5, step=0.05, description=\"weight_scale\", layout=Layout(width='100%')),\n",
        "            bias_val=widgets.FloatSlider(value=0, min=-5, max=5, step=0.05, description=\"bias_scale\", layout=Layout(width='100%')),\n",
        "        )\n",
        "        return ip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adjust the sliders to rescale the weight and bias parameters in the RNN. Observe the effect on exploding and vanishing gradients.\n",
        "\n",
        "**Parameters to try varying:**\n",
        "- `nonlinearity`: Try `lambda x: x` (identity), `F.relu`, or `torch.tanh`\n",
        "- `last_target_only`: Compare `True` vs `False`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hidden_size = 16\n",
        "nonlinearity = lambda x: x  # options: lambda x: x, F.relu, torch.tanh\n",
        "last_target_only = True\n",
        "\n",
        "rnn = RNNLayer(1, hidden_size, nonlinearity=nonlinearity)\n",
        "gv = GradientVisualizer(rnn, last_target_only)\n",
        "gv.create_visualization()\n",
        "\n",
        "# If the slider doesn't work, try calling gv.update_plots with various values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem 1.K: Making a Multi-Layer RNN\n",
        "\n",
        "![Multi-layer RNN](img/1h.png)\n",
        "\n",
        "*Figure: Multi-layer RNN architecture. Depth (vertical) vs Time (horizontal).*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.K.i: Implementing Multi-Layer Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    \"\"\"Multi-layer RNN implementation.\n",
        "    \n",
        "    Attributes:\n",
        "        input_size: Dimension of input features.\n",
        "        hidden_size: Dimension of hidden state (same for all layers).\n",
        "        num_layers: Number of stacked RNN layers.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize a multilayer RNN.\n",
        "        \n",
        "        Args:\n",
        "            input_size: Data input feature dimension.\n",
        "            hidden_size: Hidden state size (also the output feature dimension).\n",
        "            num_layers: Number of layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert num_layers >= 1\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        ##############################################################################\n",
        "        # TODO: Initialize any parameters your class needs.                          #\n",
        "        # Consider using nn.ModuleList or nn.ModuleDict.                             #\n",
        "        ##############################################################################\n",
        "        \n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Multilayer RNN forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, input_size).\n",
        "        \n",
        "        Returns:\n",
        "            last_layer_h: Outputs from the last layer (batch_size, seq_len, hidden_size).\n",
        "            last_step_h: All hidden states from the last step (num_layers, batch_size, hidden_size).\n",
        "        \"\"\"\n",
        "        ##############################################################################\n",
        "        # TODO: Implement the RNN forward step                                       #\n",
        "        ##############################################################################\n",
        "        \n",
        "        ##############################################################################\n",
        "        #                               END OF YOUR CODE                             #\n",
        "        ##############################################################################\n",
        "        return last_layer_h, last_step_h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Cases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rnn = RNN(2, 3, 1)\n",
        "rnn.load_state_dict({k: v * 0 - 0.1 for k, v in rnn.state_dict().items()})\n",
        "data = torch.FloatTensor([[[0.1, 0.15], [0.2, 0.25], [0.3, 0.35], [0.4, 0.45]], [[-0.1, -1.5], [-0.2, -2.5], [-0.3, -3.5], [-0.4, -0.45]]])\n",
        "expected_all_h = torch.FloatTensor([[[-0.1244, -0.1244, -0.1244],\n",
        "         [-0.1073, -0.1073, -0.1073],\n",
        "         [-0.1320, -0.1320, -0.1320],\n",
        "         [-0.1444, -0.1444, -0.1444]],\n",
        "        [[ 0.0599,  0.0599,  0.0599],\n",
        "         [ 0.1509,  0.1509,  0.1509],\n",
        "         [ 0.2305,  0.2305,  0.2305],\n",
        "         [-0.0840, -0.0840, -0.0840]]])\n",
        "expected_last_h = torch.FloatTensor([[[-0.1444, -0.1444, -0.1444],\n",
        "         [-0.0840, -0.0840, -0.0840]]])\n",
        "all_h, last_h = rnn(data)\n",
        "assert all_h.shape == expected_all_h.shape\n",
        "assert last_h.shape == expected_last_h.shape\n",
        "print(f'Max error all_h: {torch.max(torch.abs(expected_all_h - all_h)).item()}')\n",
        "print(f'Max error last_h: {torch.max(torch.abs(expected_last_h - last_h)).item()}')\n",
        "\n",
        "rnn = RNN(2, 3, 2)\n",
        "rnn.load_state_dict({k: v * 0 - 0.1 for k, v in rnn.state_dict().items()})\n",
        "expected_all_h = torch.FloatTensor([[[-0.0626, -0.0626, -0.0626],\n",
        "         [-0.0490, -0.0490, -0.0490],\n",
        "         [-0.0457, -0.0457, -0.0457],\n",
        "         [-0.0430, -0.0430, -0.0430]],\n",
        "        [[-0.1174, -0.1174, -0.1174],\n",
        "         [-0.1096, -0.1096, -0.1096],\n",
        "         [-0.1354, -0.1354, -0.1354],\n",
        "         [-0.0342, -0.0342, -0.0342]]])\n",
        "expected_last_h = torch.FloatTensor([[[-0.1444, -0.1444, -0.1444],\n",
        "         [-0.0840, -0.0840, -0.0840]],\n",
        "        [[-0.0430, -0.0430, -0.0430],\n",
        "         [-0.0342, -0.0342, -0.0342]]])\n",
        "all_h, last_h = rnn(data)\n",
        "assert all_h.shape == (2, 4, 3)\n",
        "assert last_h.shape == (2, 2, 3)\n",
        "print(f'Max error all_h: {torch.max(torch.abs(expected_all_h - all_h)).item()}')\n",
        "print(f'Max error last_h: {torch.max(torch.abs(expected_last_h - last_h)).item()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.K.ii: Training Your Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    num_batches: int,\n",
        "    last_timestep_only: bool,\n",
        "    seq_len: int = 10,\n",
        "    batch_size: int = 32\n",
        ") -> list[float]:\n",
        "    \"\"\"Train the RNN model on the running average task.\n",
        "    \n",
        "    Args:\n",
        "        model: The RecurrentRegressionModel to train.\n",
        "        optimizer: PyTorch optimizer.\n",
        "        num_batches: Number of training iterations.\n",
        "        last_timestep_only: Whether to compute loss only at final timestep.\n",
        "        seq_len: Length of generated sequences.\n",
        "        batch_size: Number of sequences per batch.\n",
        "    \n",
        "    Returns:\n",
        "        List of loss values for each batch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    losses = []\n",
        "    \n",
        "    from tqdm import tqdm\n",
        "    t = tqdm(range(0, num_batches))\n",
        "    for i in t:\n",
        "        data, labels = generate_batch(seq_len=seq_len, batch_size=batch_size)\n",
        "        pred, h = model(data)\n",
        "        loss = loss_fn(pred, labels, last_timestep_only)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 100 == 0:\n",
        "            t.set_description(f\"Batch: {i} Loss: {np.mean(losses[-10:])}\")\n",
        "    return losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_all(\n",
        "    hidden_size: int,\n",
        "    lr: float,\n",
        "    num_batches: int,\n",
        "    last_timestep_only: bool\n",
        ") -> tuple[list[nn.Module], list[list[float]]]:\n",
        "    \"\"\"Train and compare 1-layer and 2-layer RNN models.\"\"\"\n",
        "    input_size = 1\n",
        "    rnn_1_layer = RecurrentRegressionModel(RNN(input_size, hidden_size, 1))\n",
        "    rnn_2_layer = RecurrentRegressionModel(RNN(input_size, hidden_size, 2))\n",
        "    models = [rnn_1_layer, rnn_2_layer]\n",
        "    model_names = ['rnn_1_layer', 'rnn_2_layer']\n",
        "\n",
        "    losses = []\n",
        "    for model in models:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        loss = train(model, optimizer, num_batches, last_timestep_only)\n",
        "        losses.append(loss)\n",
        "\n",
        "    # Visualize the results\n",
        "    fig, ax1 = plt.subplots(1)\n",
        "    for loss in losses:\n",
        "        ax1.plot(loss)\n",
        "    ax1.legend(model_names)\n",
        "    plt.show()\n",
        "\n",
        "    batch_size = 4\n",
        "    x, y = generate_batch(seq_len=10, batch_size=batch_size)\n",
        "    preds_list = [model(x)[0] for model in models]\n",
        "    for i in range(batch_size):\n",
        "        fig, ax1 = plt.subplots(1)\n",
        "        ax1.plot(x[i, :, 0])\n",
        "        if last_timestep_only:\n",
        "            ax1.plot(np.arange(10), [y[i, -1].item()] * 10, 'bo')\n",
        "        else:\n",
        "            ax1.plot(y[i, :, 0], 'bo')\n",
        "        for pred in preds_list:\n",
        "            if last_timestep_only:\n",
        "                ax1.plot(np.arange(10), [pred[i, -1, 0].detach().cpu().numpy()] * 10)\n",
        "            else:\n",
        "                ax1.plot(pred[i, :, 0].detach().cpu().numpy())\n",
        "        ax1.legend(['x', 'y'] + model_names)\n",
        "        plt.show()\n",
        "    return models, losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HIDDEN_SIZE = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_BATCHES = 5000\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Train with last_timestep_only=False (predict at all timesteps)\n",
        "last_timestep_only = False\n",
        "predict_all_models, predict_all_losses = train_all(HIDDEN_SIZE, LEARNING_RATE, NUM_BATCHES, last_timestep_only)\n",
        "\n",
        "# Train with last_timestep_only=True (predict only at last timestep)\n",
        "last_timestep_only = True\n",
        "predict_one_models, predict_one_losses = train_all(HIDDEN_SIZE, LEARNING_RATE, NUM_BATCHES, last_timestep_only)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
